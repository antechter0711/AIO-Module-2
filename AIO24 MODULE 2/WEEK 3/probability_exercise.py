# -*- coding: utf-8 -*-
"""Probability Exercise

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AOC57IyKnY_kMimyVpPjxjP8R4rCi3mw
"""

import numpy as np
import pandas as pd

def create_train_data():

    data = [
        ['Sunny', 'Hot', 'High', 'Weak', 'no'],
        ['Sunny', 'Hot', 'High', 'Strong', 'no'],
        ['Overcast', 'Hot', 'High', 'Weak', 'yes'],
        ['Rain', 'Mild', 'High', 'Weak', 'yes'],
        ['Rain', 'Cool', 'Normal', 'Weak', 'yes'],
        ['Rain', 'Cool', 'Normal', 'Strong', 'no'],
        ['Overcast', 'Cool', 'Normal', 'Strong', 'yes'],
        ['Overcast', 'Mild', 'High', 'Strong', 'yes'],
        ['Overcast', 'Hot', 'Normal', 'Weak', 'yes'],
        ['Sunny', 'Cool', 'Normal', 'Weak', 'yes'],
        ['Rain', 'Mild', 'High', 'Strong', 'no']
    ]
    # Convert the numpy array to a pandas DataFrame
    data = pd.DataFrame(data, columns=['Outlook', 'Temperature', 'Humidity', 'Wind', 'target'])
    return np.array(data)

train_data = create_train_data()
print(train_data)

import numpy as np

def compute_prior_probability(train_data):
    y_unique = ['no', 'yes']
    prior_probability = np.zeros(len(y_unique))

    # Calculate the prior probabilities
    total_count = len(train_data)
    for i, y in enumerate(y_unique):
        # Count occurrences of 'y' in the last column (target variable)
        prior_probability[i] = np.sum(train_data[:, -1] == y) / total_count

    return prior_probability

# Example usage
prior_probability = compute_prior_probability(train_data)
print("P(play tennis = No)", prior_probability[0])
print("P(play tennis = Yes)", prior_probability[1])

def compute_conditional_probability(train_data):
  y_unique = ['no', 'yes']
  conditional_probability = []
  list_name = [] # Initialize list_name
  for i in range(train_data.shape[1]-1):
    x_unique = np.unique(train_data[:, i])
    list_name.append(x_unique) # Append unique values to list_name
    print (x_unique)

    x_conditional_probability = np.zeros((len(y_unique), len(x_unique)))
    for j in range(len(y_unique)):
      for k in range(len(x_unique)):
        # Use the logical AND operator instead of bitwise AND
        x_conditional_probability[j, k] = len(np.where((train_data[:, i] == x_unique[k]) & (train_data[:, 4] == y_unique[j]))[0])
    conditional_probability.append(x_conditional_probability)
  return conditional_probability, list_name # Return the updated list_name

conditional_probability, list_name = compute_conditional_probability(train_data)

def train_naive_bayes(train_data):
    # 1. Xac suat prior
    prior_probability = compute_prior_probability(train_data)

    # 2. Conditional
    conditional_probability, list_x_name = compute_conditional_probability(train_data)

    return prior_probability, conditional_probability, list_x_name

train_data = create_train_data()
list_x_name = compute_conditional_probability(train_data)

print("x1 =", list_name[0])
print("x2 =", list_name[1])
print("x3 =", list_name[2])
print("x4 =", list_name[3])

import numpy as np

def get_index_from_value(value, feature_list):
    return np.where(feature_list == value)[0][0] # Use np.where to find index in a NumPy array

# ... (rest of your code)

train_data = create_train_data()
list_x_name = compute_conditional_probability(train_data)
outlook = list_name[0]

i1 = get_index_from_value("Overcast", outlook)
i2 = get_index_from_value("Rain", outlook)
i3 = get_index_from_value("Sunny", outlook)

print(i1, i2, i3)

train_data = create_train_data()
conditional_probability, list_x_name = compute_conditional_probability(train_data)

# Compute P("Outlook"="Sunny" | "Play Tennis"="Yes")
x1 = get_index_from_value("Sunny", list_x_name[0])
print("P('Outlook'='Sunny' | 'Play Tennis'='Yes') =", np.round(conditional_probability[0][1, x1], 2))

train_data = create_train_data()
conditional_probability, list_x_name = compute_conditional_probability(train_data)

# Compute P("Outlook"="Sunny" | "Play Tennis"="No")
x1 = get_index_from_value("Sunny", list_x_name[0])
print("P('Outlook'='Sunny' | 'Play Tennis'='No') =", np.round(conditional_probability[0][0, x1], 2))

###########################
# Train Naive Bayes Model
###########################
def train_naive_bayes(train_data):
    if isinstance(train_data, np.ndarray):
        train_data = pd.DataFrame(train_data)
    # Step 1: Calculate Prior Probability
    # Ensure the last column is named 'target' before computing prior probabilities
    train_data = train_data.rename(columns={train_data.columns[-1]: 'target'})
    y_unique = ['no', 'yes']
    prior_probability = compute_prior_probability(train_data)

    # Step 2: Calculate Conditional Probability
    # No need to rename the column again here
    conditional_probability, list_name = compute_conditional_probability(train_data)

    return prior_probability, conditional_probability, list_name

def compute_prior_probability(train_data):
    total_samples = len(train_data)
    # Check if 'target' is a column in train_data and if it's a Series
    if 'target' in train_data.columns and isinstance(train_data['target'], pd.Series):
        class_counts = train_data['target'].value_counts()
        prior_probability = class_counts / total_samples
        return prior_probability
    else:
        # Handle the case where 'target' is not a valid column or Series
        print("Error: 'target' column not found or is not a Pandas Series in train_data.")
        return None  # Or raise an exception

def compute_conditional_probability(train_data):
    features = train_data.columns[:-1]  # Exclude the target column
    target = train_data['target']

    conditional_probability = {}
    for feature in features:
        conditional_probability[feature] = {}
        feature_values = train_data[feature].unique()
        for value in feature_values:
            conditional_probability[feature][value] = {}
            for target_class in target.unique():
                count = len(train_data[(train_data[feature] == value) & (train_data['target'] == target_class)])
                total = len(train_data[train_data['target'] == target_class])
                conditional_probability[feature][value][target_class] = count / total

    return conditional_probability, features.tolist()

#####################
# Prediction
#####################
def prediction_play_tennis(X, list_x_name, prior_probability, conditional_probability):
    x1 = get_index_from_value(X[0], list_name[0])
    x2 = get_index_from_value(X[1], list_name[1])
    x3 = get_index_from_value(X[2], list_name[2])
    x4 = get_index_from_value(X[3], list_name[3])

    p0 = 0
    p1 = 0

    # Calculate probability for class 0 (no)
    p0 = prior_probability['no'] * \
         conditional_probability[list_name[0]][X[0]]['no'] * \
         conditional_probability[list_name[1]][X[1]]['no'] * \
         conditional_probability[list_name[2]][X[2]]['no'] * \
         conditional_probability[list_name[3]][X[3]]['no']

    # Calculate probability for class 1 (yes)
    p1 = prior_probability['yes'] * \
         conditional_probability[list_name[0]][X[0]]['yes'] * \
         conditional_probability[list_name[1]][X[1]]['yes'] * \
         conditional_probability[list_name[2]][X[2]]['yes'] * \
         conditional_probability[list_name[3]][X[3]]['yes']

    if p0 > p1:
        y_pred = 0
    else:
        y_pred = 1

    return y_pred

# Helper function (assuming it's defined elsewhere)
def get_index_from_value(value, feature_name):
    # This function should return the index of the value in the feature's possible values
    # Implementation depends on how your data is structured
    pass

X = ['Sunny', 'Cool', 'High', 'Strong']
data = create_train_data()
prior_probability, conditional_probability, list_name = train_naive_bayes(data)
pred = prediction_play_tennis(X, list_name, prior_probability,
    conditional_probability)

if pred:
    print("Ad should go!")
else:
    print("Ad should not go!")